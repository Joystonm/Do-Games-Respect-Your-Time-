{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do Games Respect Your Time?\n",
    "## A Confidence-Aware Analysis of Time Value in Video Games\n",
    "\n",
    "**The Hook:** Time is finite. Every hour spent in a game is an hour not spent elsewhere.\n",
    "\n",
    "**The Problem:** When you see \"20 hours to beat,\" how much can you trust that number? Is it based on 5 players or 500?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"browser\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Engineering: Signal vs Noise\n",
    "\n",
    "**Philosophy:** No black-box cleaning. Every filter is documented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw dataset: 155,727 entries\n",
      "After type filter: 151,039 (-4,688)\n",
      "After completeness filter: 39,914\n",
      "After outlier removal (>62h): 39,514\n",
      "\n",
      "‚úì Final dataset: 39,514 games\n"
     ]
    }
   ],
   "source": [
    "# Load\n",
    "df = pd.read_csv('hltb_dataset.csv')\n",
    "print(f\"Raw dataset: {len(df):,} entries\")\n",
    "\n",
    "# Filter 1: Games only (no DLC)\n",
    "df = df[df['type'] == 'game'].copy()\n",
    "print(f\"After type filter: {len(df):,} (-{len(pd.read_csv('hltb_dataset.csv')) - len(df):,})\")\n",
    "\n",
    "# Filter 2: Must have main story data\n",
    "df = df[df['main_story_polled'].notna() & (df['main_story_polled'] > 0)].copy()\n",
    "df = df[df['main_story'].notna() & (df['main_story'] > 0)].copy()\n",
    "print(f\"After completeness filter: {len(df):,}\")\n",
    "\n",
    "# Filter 3: Remove extreme outliers (>99th percentile)\n",
    "time_99 = df['main_story'].quantile(0.99)\n",
    "df = df[df['main_story'] <= time_99].copy()\n",
    "print(f\"After outlier removal (>{time_99:.0f}h): {len(df):,}\")\n",
    "print(f\"\\n‚úì Final dataset: {len(df):,} games\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Method: Confidence Changes Truth\n",
    "\n",
    "**Core Innovation:** We don't treat all reported times equally.\n",
    "\n",
    "A game with 500 polls is more trustworthy than one with 5 polls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence Distribution:\n",
      "confidence_tier\n",
      "Low (<10)           29855\n",
      "Medium (10-50)       6386\n",
      "High (50-200)        2286\n",
      "Very High (>200)      987\n",
      "Name: count, dtype: int64\n",
      "\n",
      "‚ö†Ô∏è  73.9% of games have <10 polls\n"
     ]
    }
   ],
   "source": [
    "# Metric 1: Confidence Score\n",
    "df['confidence_score'] = np.log(df['main_story_polled'] + 1)\n",
    "\n",
    "# Metric 2: Time Cost (raw)\n",
    "df['time_cost'] = df['main_story']\n",
    "\n",
    "# Metric 3: Adjusted Time Cost\n",
    "df['adjusted_time_cost'] = df['time_cost'] / df['confidence_score']\n",
    "\n",
    "# Confidence tiers\n",
    "df['confidence_tier'] = pd.cut(df['main_story_polled'], \n",
    "                                bins=[0, 10, 50, 200, np.inf],\n",
    "                                labels=['Low (<10)', 'Medium (10-50)', 'High (50-200)', 'Very High (>200)'])\n",
    "\n",
    "print(\"Confidence Distribution:\")\n",
    "print(df['confidence_tier'].value_counts().sort_index())\n",
    "print(f\"\\n‚ö†Ô∏è  {100*len(df[df['main_story_polled'] < 10])/len(df):.1f}% of games have <10 polls\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Discovery: 37% of Perceived Time is Noise\n",
    "\n",
    "**The Aha Moment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median perceived time: 3.5 hours\n",
      "Median adjusted time:  2.2 hours\n",
      "\n",
      "üî• Difference: 1.3 hours (37.3%)\n",
      "\n",
      "When you account for confidence, games are 37% shorter than they appear.\n"
     ]
    }
   ],
   "source": [
    "median_raw = df['time_cost'].median()\n",
    "median_adjusted = df['adjusted_time_cost'].median()\n",
    "difference = median_raw - median_adjusted\n",
    "pct_diff = 100 * difference / median_raw\n",
    "\n",
    "print(f\"Median perceived time: {median_raw:.1f} hours\")\n",
    "print(f\"Median adjusted time:  {median_adjusted:.1f} hours\")\n",
    "print(f\"\\nüî• Difference: {difference:.1f} hours ({pct_diff:.1f}%)\")\n",
    "print(f\"\\nWhen you account for confidence, games are {pct_diff:.0f}% shorter than they appear.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualization 1: The Trust-Time Map\n",
    "\n",
    "**The hero chart.** Where does your game sit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample for performance\n",
    "sample = df.sample(min(5000, len(df)), random_state=42)\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=sample['time_cost'],\n",
    "    y=sample['confidence_score'],\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=np.sqrt(sample['main_story_polled'])/2,\n",
    "        color=sample['confidence_score'],\n",
    "        colorscale='Viridis',\n",
    "        opacity=0.6,\n",
    "        showscale=True,\n",
    "        colorbar=dict(title=\"Confidence\"),\n",
    "        line=dict(width=0)\n",
    "    ),\n",
    "    text=sample['name'],\n",
    "    hovertemplate='<b>%{text}</b><br>Hours: %{x:.1f}<br>Confidence: %{y:.2f}<extra></extra>',\n",
    "    showlegend=False\n",
    "))\n",
    "\n",
    "# Annotated regions\n",
    "fig.add_annotation(x=10, y=5, text=\"<b>Reliable Value</b><br>High confidence, low time\",\n",
    "                   showarrow=False, bgcolor=\"rgba(0,255,0,0.15)\", font=dict(size=12), borderpad=8)\n",
    "fig.add_annotation(x=80, y=2, text=\"<b>Questionable Grind</b><br>Long but uncertain\",\n",
    "                   showarrow=False, bgcolor=\"rgba(255,0,0,0.15)\", font=dict(size=12), borderpad=8)\n",
    "fig.add_annotation(x=50, y=1.5, text=\"<b>Statistical Mirage</b><br>Few reports, unreliable\",\n",
    "                   showarrow=False, bgcolor=\"rgba(255,255,0,0.15)\", font=dict(size=12), borderpad=8)\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"The Trust-Time Map: Where Confidence Meets Completion\",\n",
    "    xaxis_title=\"Main Story Hours\",\n",
    "    yaxis_title=\"Confidence Score (log polls)\",\n",
    "    template=\"plotly_white\",\n",
    "    height=700,\n",
    "    font=dict(size=13),\n",
    "    hovermode='closest'\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Genre Analysis: Rankings That Lie\n",
    "\n",
    "**Question:** Which genres change rank when confidence is modeled?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genres that RISE when confidence is modeled (underestimated):\n",
      "                      raw_median_hours  adjusted_median_hours  rank_shift  \\\n",
      "primary_genre                                                               \n",
      "Stealth                           9.17                   2.79          17   \n",
      "First-Person Shooter              6.01                   2.28           9   \n",
      "Beat Em Up                        1.78                   0.86           8   \n",
      "Third-Person Shooter              5.84                   2.24           8   \n",
      "Action Adventure                  8.02                   3.35           7   \n",
      "\n",
      "                      median_confidence  \n",
      "primary_genre                            \n",
      "Stealth                            2.40  \n",
      "First-Person Shooter               2.48  \n",
      "Beat Em Up                         2.20  \n",
      "Third-Person Shooter               2.64  \n",
      "Action Adventure                   2.64  \n",
      "\n",
      "Genres that FALL when confidence is modeled (overestimated):\n",
      "               raw_median_hours  adjusted_median_hours  rank_shift  \\\n",
      "primary_genre                                                        \n",
      "Breakout                   2.07                   2.16         -12   \n",
      "Text                       2.33                   2.20         -11   \n",
      "Exercise                   6.50                   5.92          -7   \n",
      "Flight Combat              6.43                   5.24          -7   \n",
      "Sports                     2.98                   2.30          -7   \n",
      "\n",
      "               median_confidence  \n",
      "primary_genre                     \n",
      "Breakout                     1.1  \n",
      "Text                         1.1  \n",
      "Exercise                     0.9  \n",
      "Flight Combat                1.1  \n",
      "Sports                       1.1  \n"
     ]
    }
   ],
   "source": [
    "# Normalize genres\n",
    "df['primary_genre'] = df['genres'].fillna('Unknown').str.split(',').str[0].str.strip()\n",
    "\n",
    "# Aggregate by genre (min 20 games)\n",
    "genre_stats = df.groupby('primary_genre').agg({\n",
    "    'time_cost': ['median', 'count'],\n",
    "    'confidence_score': 'median',\n",
    "    'adjusted_time_cost': 'median',\n",
    "    'main_story_polled': 'sum'\n",
    "}).round(2)\n",
    "\n",
    "genre_stats.columns = ['raw_median_hours', 'game_count', 'median_confidence', 'adjusted_median_hours', 'total_polls']\n",
    "genre_stats = genre_stats[genre_stats['game_count'] >= 20].copy()\n",
    "\n",
    "# Calculate rank shift\n",
    "genre_stats['raw_rank'] = genre_stats['raw_median_hours'].rank()\n",
    "genre_stats['adjusted_rank'] = genre_stats['adjusted_median_hours'].rank()\n",
    "genre_stats['rank_shift'] = (genre_stats['raw_rank'] - genre_stats['adjusted_rank']).astype(int)\n",
    "\n",
    "# Show biggest movers\n",
    "print(\"Genres that RISE when confidence is modeled (underestimated):\")\n",
    "print(genre_stats.nlargest(5, 'rank_shift')[['raw_median_hours', 'adjusted_median_hours', 'rank_shift', 'median_confidence']])\n",
    "\n",
    "print(\"\\nGenres that FALL when confidence is modeled (overestimated):\")\n",
    "print(genre_stats.nsmallest(5, 'rank_shift')[['raw_median_hours', 'adjusted_median_hours', 'rank_shift', 'median_confidence']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualization 2: Genre Reliability Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_genres = genre_stats.nsmallest(15, 'adjusted_median_hours')\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    y=top_genres.index,\n",
    "    x=top_genres['raw_median_hours'],\n",
    "    name='Perceived (raw)',\n",
    "    orientation='h',\n",
    "    marker=dict(color='lightcoral', opacity=0.7)\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    y=top_genres.index,\n",
    "    x=top_genres['adjusted_median_hours'],\n",
    "    name='Actual (confidence-adjusted)',\n",
    "    orientation='h',\n",
    "    marker=dict(color='steelblue')\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Genre Rankings: How Confidence Changes Truth<br><sub>Top 15 genres by adjusted time</sub>\",\n",
    "    xaxis_title=\"Median Hours to Complete\",\n",
    "    yaxis_title=\"\",\n",
    "    barmode='group',\n",
    "    template=\"plotly_white\",\n",
    "    height=600,\n",
    "    font=dict(size=12),\n",
    "    legend=dict(x=0.7, y=0.02)\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualization 3: The Illusion of Length\n",
    "\n",
    "**Games that seem longer than they are** (low confidence = high uncertainty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find games with biggest perception gap\n",
    "illusion = df[df['main_story_polled'] < 20].copy()\n",
    "illusion['perception_gap'] = illusion['time_cost'] - illusion['adjusted_time_cost']\n",
    "illusion = illusion.nlargest(30, 'perception_gap')\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "# Perfect accuracy line\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=[0, illusion['time_cost'].max()],\n",
    "    y=[0, illusion['time_cost'].max()],\n",
    "    mode='lines',\n",
    "    line=dict(dash='dash', color='gray', width=2),\n",
    "    name='Perfect accuracy',\n",
    "    showlegend=True\n",
    "))\n",
    "\n",
    "# Games\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=illusion['time_cost'],\n",
    "    y=illusion['adjusted_time_cost'],\n",
    "    mode='markers',\n",
    "    marker=dict(size=10, color='red', opacity=0.7),\n",
    "    text=illusion['name'],\n",
    "    hovertemplate='<b>%{text}</b><br>Perceived: %{x:.1f}h<br>Adjusted: %{y:.1f}h<extra></extra>',\n",
    "    name='Low-confidence games',\n",
    "    showlegend=True\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"The Illusion of Length: Games That Seem Longer Than They Are<br><sub>Games with <20 polls and largest perception gaps</sub>\",\n",
    "    xaxis_title=\"Perceived Time (raw hours)\",\n",
    "    yaxis_title=\"Adjusted Time (confidence-weighted hours)\",\n",
    "    template=\"plotly_white\",\n",
    "    height=600,\n",
    "    font=dict(size=12)\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(\"\\nTop 10 games with biggest illusion:\")\n",
    "print(illusion[['name', 'time_cost', 'adjusted_time_cost', 'main_story_polled', 'perception_gap']].head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. The Meaning: How This Changes Decisions\n",
    "\n",
    "**Three takeaways:**\n",
    "\n",
    "1. **74% of games have <10 polls** ‚Äî most length estimates are statistically unreliable\n",
    "\n",
    "2. **37% of perceived time is noise** ‚Äî when confidence is modeled, games are shorter than they appear\n",
    "\n",
    "3. **Genre rankings shift dramatically** ‚Äî some genres are systematically over/underestimated\n",
    "\n",
    "**The unmistakable insight:**\n",
    "\n",
    "> When you see a game's completion time, you're not seeing truth ‚Äî you're seeing a confidence-weighted average that most platforms ignore. The games that \"respect your time\" might just be the ones with enough players to report accurately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Methodological Notes\n",
    "\n",
    "**Confidence Score:** `log(polls + 1)` ‚Äî logarithmic scaling prevents extreme poll counts from dominating\n",
    "\n",
    "**Adjusted Time Cost:** `time / confidence` ‚Äî penalizes low-confidence estimates\n",
    "\n",
    "**Outlier Removal:** 99th percentile cutoff (games >100h) to prevent long-tail distortion\n",
    "\n",
    "**Genre Normalization:** Primary genre only (first listed) to avoid double-counting\n",
    "\n",
    "**Sample Size:** 39,514 games after cleaning (1% loss from raw dataset)\n",
    "\n",
    "**Limitations:** \n",
    "- Self-reported data (selection bias)\n",
    "- No skill/difficulty adjustment\n",
    "- Platform differences not modeled\n",
    "- Confidence score is a proxy, not ground truth"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
